#!/usr/bin/env python3
"""
Test the new indexer functions without model dependencies
"""

import tempfile
import sys
from pathlib import Path

# Add worker directory to path
sys.path.append('/Users/kritimaheshwari/Desktop/lucidfiles/worker')

def test_parse_file_to_text_logic():
    """Test the parse_file_to_text logic without importing indexer directly."""
    print("=== Testing parse_file_to_text Logic ===\n")
    
    try:
        import parsers
        
        def mock_parse_file_to_text(file_path):
            """Recreate the logic from indexer.parse_file_to_text()"""
            file_path = Path(file_path)
            
            if not file_path.exists():
                print(f"Warning: File not found: {file_path}")
                return ""
            
            extension = file_path.suffix.lower()
            
            try:
                # Use parsers.parse_file which handles different extensions
                return parsers.parse_file(file_path)
            except Exception as e:
                print(f"Warning: Failed to parse {file_path}: {e}")
                return ""
        
        # Create test files
        test_files = []
        
        # 1. Text file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            content = "This is a test text file.\nWith multiple lines."
            f.write(content)
            test_files.append((Path(f.name), 'txt', content))
        
        # 2. Python file (should be treated as text)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
            content = "# Python file\nprint('Hello, world!')\n"
            f.write(content)
            test_files.append((Path(f.name), 'py', content))
        
        # 3. Unsupported file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.xyz', delete=False, encoding='utf-8') as f:
            content = "Unsupported file content"
            f.write(content)
            test_files.append((Path(f.name), 'xyz', content))
        
        # Test parsing
        for file_path, file_type, expected_content in test_files:
            try:
                parsed_text = mock_parse_file_to_text(file_path)
                
                if file_type == 'xyz':
                    # Should return empty string for unsupported types
                    success = parsed_text == ""
                    print(f"‚úì {file_type} file: {'Empty string returned' if success else 'Unexpected content'}")
                else:
                    # Should return the content
                    success = parsed_text.strip() == expected_content.strip()
                    print(f"‚úì {file_type} file: {'Parsed correctly' if success else 'Parse failed'}")
                    print(f"  Content: {parsed_text[:50]}{'...' if len(parsed_text) > 50 else ''}")
                
            except Exception as e:
                print(f"‚úó {file_type} file: Error - {e}")
            
            print()
        
        # Clean up
        for file_path, _, _ in test_files:
            try:
                file_path.unlink()
            except:
                pass
        
        return True
        
    except Exception as e:
        print(f"‚úó Parse file to text logic test failed: {e}")
        return False

def test_index_files_logic():
    """Test the index_files logic without model dependencies."""
    print("=== Testing index_files Logic ===\n")
    
    try:
        import parsers
        import chunker
        import utils
        
        def mock_index_files(file_paths):
            """Recreate the logic from indexer.index_files() without embeddings"""
            
            if not file_paths:
                return {"files_indexed": 0, "chunks_indexed": 0, "points": 0}
            
            result = {"files_indexed": 0, "chunks_indexed": 0, "points": 0}
            
            for file_path in file_paths:
                try:
                    file_path = Path(file_path)
                    
                    if not file_path.exists():
                        print(f"Warning: File not found: {file_path}")
                        continue
                    
                    # Parse file to text
                    text = parsers.parse_file(file_path)
                    if not text or not text.strip():
                        print(f"Warning: No text extracted from {file_path}")
                        continue
                    
                    # Chunk the text
                    chunks = chunker.chunk_text(text, chunk_size=500, overlap=50)
                    if not chunks:
                        print(f"Warning: No chunks created from {file_path}")
                        continue
                    
                    # Create file metadata
                    file_hash = utils.sha256_file(file_path)
                    
                    # Simulate processing chunks
                    for i, chunk_text in enumerate(chunks):
                        if chunk_text.strip():
                            result["points"] += 1
                    
                    result["files_indexed"] += 1
                    result["chunks_indexed"] += len(chunks)
                    
                    print(f"‚úì Processed {file_path.name}: {len(chunks)} chunks")
                    
                except Exception as e:
                    print(f"‚úó Failed to process {file_path}: {e}")
            
            return result
        
        # Create sample files for indexing
        test_files = []
        
        # Document 1
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            content = """
            Introduction to Machine Learning
            
            Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can analyze data and make predictions or decisions.
            
            Key concepts include supervised learning, unsupervised learning, and reinforcement learning. Each approach has different applications and use cases in various domains.
            """
            f.write(content.strip())
            test_files.append(Path(f.name))
        
        # Document 2
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
            content = """
            # Vector Databases
            
            Vector databases are specialized databases designed to store and query high-dimensional vectors efficiently. They are essential for modern AI applications that work with embeddings.
            
            ## Features
            - Fast similarity search
            - Scalable architecture
            - Integration with ML pipelines
            
            Popular examples include Qdrant, Pinecone, and Weaviate.
            """
            f.write(content.strip())
            test_files.append(Path(f.name))
        
        # Document 3 - Short document
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            content = "Short document with minimal content."
            f.write(content)
            test_files.append(Path(f.name))
        
        try:
            # Convert paths to strings
            file_paths = [str(p) for p in test_files]
            
            print(f"Processing {len(file_paths)} files...")
            for i, path in enumerate(file_paths, 1):
                print(f"  {i}. {Path(path).name}")
            print()
            
            # Run indexing logic
            result = mock_index_files(file_paths)
            
            print("Processing Results:")
            print(f"  Files indexed: {result['files_indexed']}")
            print(f"  Chunks created: {result['chunks_indexed']}")
            print(f"  Points that would be stored: {result['points']}")
            print()
            
            # Verify results
            success = True
            if result['files_indexed'] > 0:
                print("‚úì Successfully processed files")
            else:
                print("‚ö†Ô∏è  No files were processed")
                success = False
            
            if result['chunks_indexed'] > 0:
                print("‚úì Text chunking worked")
            else:
                print("‚ö†Ô∏è  No chunks were created")
                success = False
            
            if result['points'] > 0:
                print("‚úì Chunk processing worked")
            else:
                print("‚ö†Ô∏è  No chunks were processed")
                success = False
            
            # Test edge cases
            print("\nTesting edge cases:")
            
            # Non-existent file
            edge_result = mock_index_files(["/non/existent/file.txt"])
            print(f"  Non-existent file: {edge_result['files_indexed']} files indexed (should be 0)")
            
            # Empty list
            empty_result = mock_index_files([])
            print(f"  Empty list: {empty_result['files_indexed']} files indexed (should be 0)")
            
            return success
            
        finally:
            # Clean up
            print("\nCleaning up test files...")
            for file_path in test_files:
                try:
                    file_path.unlink()
                    print(f"  Removed: {file_path.name}")
                except Exception as e:
                    print(f"  Failed to remove {file_path.name}: {e}")
        
    except Exception as e:
        print(f"‚úó Index files logic test failed: {e}")
        return False

def main():
    """Run all tests."""
    print("üß™ Testing Extended Indexer Logic (Without Model Dependencies)\n")
    
    tests = [
        test_parse_file_to_text_logic,
        test_index_files_logic
    ]
    
    passed = 0
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"‚úó Test {test.__name__} crashed: {e}")
    
    print(f"\nüìä Results: {passed}/{len(tests)} tests passed")
    
    if passed == len(tests):
        print("‚úÖ All indexer logic tests passed!")
        print("\nThe new functions in indexer.py should work correctly")
        print("when the SentenceTransformers dependency issue is resolved.")
    else:
        print("‚ö†Ô∏è Some tests failed - check the output above")

if __name__ == "__main__":
    main()
